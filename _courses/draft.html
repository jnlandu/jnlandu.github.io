<div class="course-container">
    <h1 class="h5 yellow">Optimization for Machine Learning</h1>
    <p>
        Many problems in statistical learning consist of finding the best set of parameters or the best functions given some data. These are estimation problems.
        These problems, encountered almost everywhere in Machine Learning and Deep Learning, can easily be formulated as 
        optimization problems. These formulations help to understand the performance of learning algorithms.
        In this course, classical convex optimization theory was presented. Classical gradient methods and their variants were discussed, as well as variance-reduced methods and accelerated gradient methods.
        
        The purpose of the course was to present the theoretical formulation of convex problems and the asymptotic behaviors of (Stochastic) Gradient methods. For instance, it was shown that 
        although Stochastic Gradient Descent (SGD) is efficient—because it does not make use of the full dataset—it never converges to the optimal solution without restrictive assumptions. We also implemented these algorithms from scratch and worked on theoretical exercises.
        Please refer to the materials <a class="yellow" href="optim">here</a>. The course was given by Dr. <a class="yellow" href="https://www.linkedin.com/in/lionel-ngoupeyou-tondji-057a25128/?originalSubdomain=de">Lionel Tondji</a>, 
        a former student of AMMI. Find the goodbye photos <a class="yellow" href="ammi-photos/optim">here</a>.
    </p>    
  </div>
  <div class="course-container">
    <h1 class="h5 yellow">Computer Vision</h1>
    <p>
      This course was split into two parts: the first part was an introduction to computer vision, taught by Dr. 
      <a class="yellow" href="https://lvdmaaten.github.io/">Laurens van der Maaten</a> 
      from <a class="yellow" href="https://ai.meta.com/">Meta AI</a>, and co-inventor of 
      <a class="yellow" href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-SNE</a>. 
      The second part focused on object detection and was given by Dr. <a class="yellow" href="https://scholar.google.fr/citations?user=cLPaHcIAAAAJ&hl=en">Natalia Neverova</a>
       from <a class="yellow" href="https://ai.meta.com/">Meta AI</a>.
      
      In the first part, we discussed CNNs, RNNs, U-Net, and implemented these architectures and related ones from scratch. 
      See my <a class="yellow" href="git">repo</a> for the code. The second part focused on detection. We covered vision models,
       diffusion models, distillation, video understanding, and more. The main tool used in this part was FAIR's <a class="yellow" href="https://ai.meta.com/tools/detectron2/">Detectron2</a>, with which we experimented using vision models like R-CNN, Mask R-CNN, Fast R-CNN, and RPN. Please check out my series of tutorials on <a class="yellow" href="https://en.wikipedia.org/wiki/Object_detection">Object Detection with Detectron2</a>.
    </p>
    

  </div>