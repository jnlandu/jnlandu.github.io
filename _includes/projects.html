<div class=" project-content-container"  style="animation-delay: 0s;">
    <h1 class="h5 text-brand">Foundation of Machine Learning</h1>
    <p class="h6 text-white course-summary">
      This course was provided by Prof. <a class="text-brand" href="https://scholar.google.com/citations?user=5wuvTfoAAAAJ&hl=en">Moustapha Cisse</a>. The course had two parts: Statistical Learning Theory and an introduction to Deep Learning. In the first part, we explored theoretical aspects of machine learning, particularly statistical learning theory, covering both supervised and unsupervised learning. We discussed the ERM problem, bias-variance tradeoff, overfitting and underfitting, linear regression, classification, generalized linear models, and dimensionality reduction techniques such as:
      <ul class="h6 text-white">
        <li>Random projections</li>
        <li>Principal Component Analysis</li>
        <li>Canonical Correlation Analysis</li>
      </ul>
      In the second part, we covered neural networks, weight initialization and normalization, CNNs, RNNs, attention mechanisms, and transformers.<br>
      Please find <a class="text-brand" href="here">here</a> the materials I used and some code I implemented. Also, check out my tutorials below.
    </p>
</div>
<div class=" project-content-container"  style="animation-delay: 0s;">
    <h1 class="h5 text-brand">Kernel Methods for Machine Learning</h1>
    <p>
        This course was given by <a class="text-brand" href="https://jpvert.github.io/">J.P. Vert</a> from <a class="text-brand" href="https://www.owkin.com/">Owkin</a>, with T.As. Romain Menegeaux and Juliette-Marie from Inria.
        The course covered basic concepts of machine learning in high dimensions and the importance of regularization. We also discussed Support Vector Machines (SVM).
        We studied in detail high-dimensional linear models regularized by the Euclidean norm or the Manhattan norm, including ridge and lasso
        regression, as well as ridge classification. One of the key ideas of this course
        was to demonstrate that some learning problems, which can be complex to solve in finite-dimensional Euclidean space, can be easily addressed
        in infinite-dimensional spaces, provided a good feature mapping. The kernel is simply a similarity function between data points. We were then shown how positive kernels, via the representer theorem, transform the ERM problem 
        in Euclidean space into an ERM problem in large-dimensional space. Kernels allow for transforming linear models into non-linear models,
        applicable even to non-vectorial data such as strings and graphs. We also discussed 
        Reproducing Kernel Hilbert Spaces (RKHS) and related theorems. Please find my <a class="text-brand" href="codes">code</a> and the course page <a class="text-brand" href="here">here</a>.
    </p>

</div>